{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_to_onnx.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0jOHyjhhMNB",
        "outputId": "f6177e4e-db5c-4108-8c28-ed8e5339c074",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\"\"\"\n",
        "Authors\n",
        " * Tulasi Ram\n",
        "\"\"\"\n",
        "\n",
        "!pip install onnx\n",
        "!pip install onnxruntime\n",
        "!pip install pip install git+https://github.com/onnx/onnx-tensorflow.git\n",
        "\n",
        "\n",
        "import gdown\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torchvision import models\n",
        "from torchvision.models.vgg import model_urls\n",
        "from collections import namedtuple\n",
        "from collections import OrderedDict\n",
        "import onnx\n",
        "import onnxruntime\n",
        "from onnx_tf.backend import prepare"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: onnx in /usr/local/lib/python3.6/dist-packages (1.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from onnx) (1.15.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from onnx) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from onnx) (1.18.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx) (3.7.4.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->onnx) (50.3.2)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.6/dist-packages (1.5.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from onnxruntime) (3.12.4)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.6/dist-packages (from onnxruntime) (1.18.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->onnxruntime) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->onnxruntime) (50.3.2)\n",
            "Collecting git+https://github.com/onnx/onnx-tensorflow.git\n",
            "  Cloning https://github.com/onnx/onnx-tensorflow.git to /tmp/pip-req-build-5kdq50y9\n",
            "  Running command git clone -q https://github.com/onnx/onnx-tensorflow.git /tmp/pip-req-build-5kdq50y9\n",
            "Requirement already satisfied (use --upgrade to upgrade): onnx-tf==1.6.0 from git+https://github.com/onnx/onnx-tensorflow.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (19.3.1)\n",
            "Requirement already satisfied: install in /usr/local/lib/python3.6/dist-packages (1.3.4)\n",
            "Requirement already satisfied: onnx>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from onnx-tf==1.6.0) (1.8.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from onnx-tf==1.6.0) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from onnx>=1.6.0->onnx-tf==1.6.0) (1.15.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from onnx>=1.6.0->onnx-tf==1.6.0) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from onnx>=1.6.0->onnx-tf==1.6.0) (1.18.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx>=1.6.0->onnx-tf==1.6.0) (3.7.4.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->onnx>=1.6.0->onnx-tf==1.6.0) (50.3.2)\n",
            "Building wheels for collected packages: onnx-tf\n",
            "  Building wheel for onnx-tf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for onnx-tf: filename=onnx_tf-1.6.0-cp36-none-any.whl size=204404 sha256=3f561a23dc7b5ff4b28168738e7a07c4c303a28f31c0536bfbc2bcd210a633d9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wc1g0l2z/wheels/54/24/31/8873b33d2d560efdfa1ed6f346df67ef793b1897358705a480\n",
            "Successfully built onnx-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3Q_RNE9hHoL"
      },
      "source": [
        "def copyStateDict(state_dict):\n",
        "    if list(state_dict.keys())[0].startswith(\"module\"):\n",
        "        start_idx = 1\n",
        "    else:\n",
        "        start_idx = 0\n",
        "    new_state_dict = OrderedDict()\n",
        "    for k, v in state_dict.items():\n",
        "        name = \".\".join(k.split(\".\")[start_idx:])\n",
        "        new_state_dict[name] = v\n",
        "    return new_state_dict\n",
        "\n",
        "def init_weights(modules):\n",
        "    for m in modules:\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            init.xavier_uniform_(m.weight.data)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.zero_()\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            m.weight.data.fill_(1)\n",
        "            m.bias.data.zero_()\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            m.weight.data.normal_(0, 0.01)\n",
        "            m.bias.data.zero_()\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV9nYbloikdE"
      },
      "source": [
        "class vgg16_bn(torch.nn.Module):\n",
        "    def __init__(self, pretrained=True, freeze=True):\n",
        "        super(vgg16_bn, self).__init__()\n",
        "        model_urls['vgg16_bn'] = model_urls['vgg16_bn'].replace('https://', 'http://')\n",
        "        vgg_pretrained_features = models.vgg16_bn(pretrained=pretrained).features\n",
        "        self.slice1 = torch.nn.Sequential()\n",
        "        self.slice2 = torch.nn.Sequential()\n",
        "        self.slice3 = torch.nn.Sequential()\n",
        "        self.slice4 = torch.nn.Sequential()\n",
        "        self.slice5 = torch.nn.Sequential()\n",
        "        for x in range(12):         # conv2_2\n",
        "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(12, 19):         # conv3_3\n",
        "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(19, 29):         # conv4_3\n",
        "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(29, 39):         # conv5_3\n",
        "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
        "\n",
        "        # fc6, fc7 without atrous conv\n",
        "        self.slice5 = torch.nn.Sequential(\n",
        "                nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "                nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6),\n",
        "                nn.Conv2d(1024, 1024, kernel_size=1)\n",
        "        )\n",
        "\n",
        "        if not pretrained:\n",
        "            init_weights(self.slice1.modules())\n",
        "            init_weights(self.slice2.modules())\n",
        "            init_weights(self.slice3.modules())\n",
        "            init_weights(self.slice4.modules())\n",
        "\n",
        "        init_weights(self.slice5.modules())        # no pretrained model for fc6 and fc7\n",
        "\n",
        "        if freeze:\n",
        "            for param in self.slice1.parameters():      # only first conv\n",
        "                param.requires_grad= False\n",
        "\n",
        "    def forward(self, X):\n",
        "        h = self.slice1(X)\n",
        "        h_relu2_2 = h\n",
        "        h = self.slice2(h)\n",
        "        h_relu3_2 = h\n",
        "        h = self.slice3(h)\n",
        "        h_relu4_3 = h\n",
        "        h = self.slice4(h)\n",
        "        h_relu5_3 = h\n",
        "        h = self.slice5(h)\n",
        "        h_fc7 = h\n",
        "        vgg_outputs = namedtuple(\"VggOutputs\", ['fc7', 'relu5_3', 'relu4_3', 'relu3_2', 'relu2_2'])\n",
        "        out = vgg_outputs(h_fc7, h_relu5_3, h_relu4_3, h_relu3_2, h_relu2_2)\n",
        "        return out\n",
        "\n",
        "class double_conv(nn.Module):\n",
        "    def __init__(self, in_ch, mid_ch, out_ch):\n",
        "        super(double_conv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch + mid_ch, mid_ch, kernel_size=1),\n",
        "            nn.BatchNorm2d(mid_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybz4MHHIio-Q"
      },
      "source": [
        "class CRAFT(nn.Module):\n",
        "    def __init__(self, pretrained=False, freeze=False):\n",
        "        super(CRAFT, self).__init__()\n",
        "\n",
        "        \"\"\" Base network \"\"\"\n",
        "        self.basenet = vgg16_bn(pretrained, freeze)\n",
        "\n",
        "        \"\"\" U network \"\"\"\n",
        "        self.upconv1 = double_conv(1024, 512, 256)\n",
        "        self.upconv2 = double_conv(512, 256, 128)\n",
        "        self.upconv3 = double_conv(256, 128, 64)\n",
        "        self.upconv4 = double_conv(128, 64, 32)\n",
        "\n",
        "        num_class = 2\n",
        "        self.conv_cls = nn.Sequential(\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 16, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(16, 16, kernel_size=1), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(16, num_class, kernel_size=1),\n",
        "        )\n",
        "\n",
        "        init_weights(self.upconv1.modules())\n",
        "        init_weights(self.upconv2.modules())\n",
        "        init_weights(self.upconv3.modules())\n",
        "        init_weights(self.upconv4.modules())\n",
        "        init_weights(self.conv_cls.modules())\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\" Base network \"\"\"\n",
        "        sources = self.basenet(x)\n",
        "\n",
        "        \"\"\" U network \"\"\"\n",
        "        y = torch.cat([sources[0], sources[1]], dim=1)\n",
        "        y = self.upconv1(y)\n",
        "\n",
        "        y = F.interpolate(y, size=sources[2].size()[2:], mode='bilinear', align_corners=False)\n",
        "        y = torch.cat([y, sources[2]], dim=1)\n",
        "        y = self.upconv2(y)\n",
        "\n",
        "        y = F.interpolate(y, size=sources[3].size()[2:], mode='bilinear', align_corners=False)\n",
        "        y = torch.cat([y, sources[3]], dim=1)\n",
        "        y = self.upconv3(y)\n",
        "\n",
        "        y = F.interpolate(y, size=sources[4].size()[2:], mode='bilinear', align_corners=False)\n",
        "        y = torch.cat([y, sources[4]], dim=1)\n",
        "        feature = self.upconv4(y)\n",
        "\n",
        "        y = self.conv_cls(feature)\n",
        "\n",
        "        return y.permute(0,2,3,1), feature"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OEhpCyujWCK"
      },
      "source": [
        "# Link to the pretrained model.\n",
        "# https://drive.google.com/uc?export=download&id=1Jk4eGD7crsqCCg9C9VjCLkMN3ze8kutZ"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9ndN1rpirzF",
        "outputId": "167ac3e9-5b38-441c-8705-41dc19d1aedb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "net = CRAFT()\n",
        "net.load_state_dict(copyStateDict(torch.load('craft_mlt_25k.pth', map_location='cpu')))\n",
        "#net.load_state_dict(copyStateDict(torch.load('.EasyOCR/model/craft_mlt_25k.pth', map_location='cuda')))\n",
        "#net = torch.nn.DataParallel(net).to('cuda')\n",
        "#cudnn.benchmark = False\n",
        "print(\"Model loaded\")\n",
        "net.eval()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model loaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CRAFT(\n",
              "  (basenet): vgg16_bn(\n",
              "    (slice1): Sequential(\n",
              "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (9): ReLU(inplace=True)\n",
              "      (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (slice2): Sequential(\n",
              "      (12): ReLU(inplace=True)\n",
              "      (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (16): ReLU(inplace=True)\n",
              "      (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (slice3): Sequential(\n",
              "      (19): ReLU(inplace=True)\n",
              "      (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (22): ReLU(inplace=True)\n",
              "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (26): ReLU(inplace=True)\n",
              "      (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (slice4): Sequential(\n",
              "      (29): ReLU(inplace=True)\n",
              "      (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (32): ReLU(inplace=True)\n",
              "      (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (36): ReLU(inplace=True)\n",
              "      (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (slice5): Sequential(\n",
              "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
              "      (1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
              "      (2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (upconv1): double_conv(\n",
              "    (conv): Sequential(\n",
              "      (0): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (upconv2): double_conv(\n",
              "    (conv): Sequential(\n",
              "      (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (upconv3): double_conv(\n",
              "    (conv): Sequential(\n",
              "      (0): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (upconv4): double_conv(\n",
              "    (conv): Sequential(\n",
              "      (0): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (conv_cls): Sequential(\n",
              "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(16, 2, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBUdAMeIiuW-",
        "outputId": "19a1677e-377b-4812-b1c2-56ed17386763",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "batch_size = 1\n",
        "# Input to the model\n",
        "x = torch.randn(batch_size, 3, 224, 224, requires_grad=True)\n",
        "onnx_runtime_input = x.detach().numpy()\n",
        "t1 = datetime.now()\n",
        "torch_out = net(x)\n",
        "t2 = datetime.now()\n",
        "print(\"Time taken for Pytoch model\", str(t2-t1))\n",
        "store_out = torch_out[0].detach().numpy()\n",
        "print(\"Output size\", torch_out[0].size())\n",
        "print(\"Model ran sucesfully\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time taken for Pytoch model 0:00:00.853400\n",
            "Output size torch.Size([1, 112, 112, 2])\n",
            "Model ran sucesfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpJeKd8plDHj",
        "outputId": "5cc5d779-5a1e-4ac2-b86c-cfb9a671c514",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Export the model\n",
        "torch.onnx.export(net,               # model being run\n",
        "                  x,                         # model input (or a tuple for multiple inputs)\n",
        "                  \"craft.onnx\",   # where to save the model (can be a file or file-like object)\n",
        "                  export_params=True,        # store the trained parameter weights inside the model file\n",
        "                  opset_version=10,          # the ONNX version to export the model to\n",
        "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
        "                  input_names = ['input'],   # the model's input names\n",
        "                  output_names = ['output'], # the model's output names\n",
        "                  dynamic_axes={'input' : {0 : 'batch_size',\n",
        "                                           2 : 'width',\n",
        "                                           3 : 'height'},    # variable lenght axes\n",
        "                                'output' : {0 : 'batch_size',\n",
        "                                            1 : 'width',\n",
        "                                            2: 'height'}})\n",
        "print(\"Model converted succesfully\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/onnx/symbolic_helper.py:267: UserWarning: You are trying to export the model with onnx:Resize for ONNX opset version 10. This operator might cause results to not match the expected results by PyTorch.\n",
            "ONNX's Upsample/Resize operator did not match Pytorch's Interpolation until opset 11. Attributes to determine how to transform the input were added in onnx:Resize in opset 11 to support Pytorch's behavior (like coordinate_transformation_mode and nearest_mode).\n",
            "We recommend using opset 11 and above for models using this operator. \n",
            "  \"\" + str(_export_onnx_opset_version) + \". \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model converted succesfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RF_HysIMlGCG",
        "outputId": "93a846cd-72d8-4eaa-e952-a14ae32421ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "onnx_model = onnx.load(\"craft.onnx\")\n",
        "onnx.checker.check_model(onnx_model)\n",
        "print(\"Model checked succesfully\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model checked succesfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfuqPQ6UlJTJ",
        "outputId": "2e00a119-9558-4348-a81a-1a246bf4f65b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ort_session = onnxruntime.InferenceSession(\"craft.onnx\")\n",
        "\n",
        "def to_numpy(tensor):\n",
        "    print(tensor)\n",
        "    return tensor.detach().cpu().numpy()\n",
        "\n",
        "# # compute ONNX Runtime output prediction\n",
        "ort_inputs = {ort_session.get_inputs()[0].name: onnx_runtime_input}\n",
        "t1 = datetime.now()\n",
        "ort_outs = ort_session.run(None, ort_inputs)\n",
        "t2 = datetime.now()\n",
        "print(\"Time taken for Onnx model\", str(t2-t1))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time taken for Onnx model 0:00:00.543383\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNuTGsR_lPqH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}